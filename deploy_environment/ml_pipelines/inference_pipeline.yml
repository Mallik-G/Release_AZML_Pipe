pipeline:
    name: MyPipeline
    parameters: # pipeline execution level parameters which can be changed run by run
        preprocess_num_buckets:
            type: int
            default: 5
    data_references:
        RawDataPath:
            datastore: inputstore
            path_on_datastore: rawdata
        ProcessedDataPath:
            datastore: outputstore
            path_on_datastore: processed
        PredictionsPath:
            datastore: predictions
            path_on_datastore: predictions
    default_compute: cpu # Value comes from arm template creation
    steps:
        Step1:
            compute: cpu
            runconfig: "../src/Inference_Pipeline/run_config.yml" #in relation to current working directory
            inputs:
                InputData:
                    source: RawDataPath
                    type: mount
            outputs:
                OutputData:
                    destination: ProcessedData #made up name for reference in future steps
                    datastore: outputstore
                    type: mount
            parameters:
                num_buckets:
                    source: preprocess_num_buckets
            arguments: #permanently affixed to a pipeline
            - "input_mount_path"
            - input:InputData #colon notation says; look at the yaml area before colon and find sub item after colon.
            - "output_mount_path"
            - output:OutputData
            - "num_buckets"
            - parameter:num_buckets
            python_script_step:
                name: "Featurize"
                script_name: "pre_process.py" #in relation to source_directory
                source_directory: "../src/Inference_Pipeline/" #in relation to current working directory
                allow_reuse: true
        Step2:
            compute: cpu
            runconfig: "../src/Inference_Pipeline/run_config.yml" #in relation to current working directory
            inputs:
                InputData:
                    source: ProcessedData #from the made up name in step1's output data
                    type: mount
            outputs:
                OutputData:
                    destination: Predictions #made up name for reference in future steps
                    datastore: predictions
                    type: mount
            arguments: #permanently affixed to a pipeline
            - "input_mount_path"
            - input:InputData #colon notation says; look at the yaml area before colon and find sub item after colon.
            - "output_mount_path"
            - output:OutputData
            python_script_step:
                name: "Score"
                script_name: "score.py" #in relation to source_directory
                source_directory: "../src/Inference_Pipeline/" #in relation to current working directory
                allow_reuse: true
